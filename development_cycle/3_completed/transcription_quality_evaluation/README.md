# Transcription Quality Evaluation Feature - README

This document outlines the Transcription Quality Evaluation feature.

## 1. Goals
- Provide a robust mechanism to evaluate the quality of speech-to-text transcriptions.
- Calculate standard industry metrics: Word Error Rate (WER) and Character Error Rate (CER).
- Offer a detailed breakdown of word errors (insertions, deletions, substitutions).
- Enable comparison between ground truth transcriptions and hypotheses generated by ASR models.
- Output results in a structured, machine-readable format (JSON) for easy consumption and potential frontend display, alongside a human-readable console summary.

## 2. Key Requirements
- **Input:**
    - Ground Truth: A JSON file containing a list of records, each with `audio_file_name` and `ground_truth_text`.
    - Hypotheses:
        - A directory containing individual `.txt` files (filename matching ground truth `audio_file_name`).
        - OR a single JSON file (either a list of records with `audio_file_name` and `text`, or a dictionary mapping filenames to text).
- **Processing:**
    - Text Normalization: Apply normalization (lowercase, punctuation removal, contraction expansion) to both ground truth and hypothesis texts before comparison.
    - Metrics Calculation: Use the `jiwer` library for accurate WER, CER, and detailed error counts (hits, substitutions, deletions, insertions).
- **Output:**
    - **JSON Report File (`--output_file <filename>.json`):**
        - `global_metrics`:
            - `wer_percentage`: Overall WER across all files.
            - `insertion_rate_percentage`: Overall insertion rate.
            - `substitution_rate_percentage`: Overall substitution rate.
            - `deletion_rate_percentage`: Overall deletion rate.
            - `total_ground_truth_words`: Total words in ground truth across all evaluated files.
            - `total_insertions`, `total_substitutions`, `total_deletions`, `total_hits`: Aggregate counts.
            - `average_cer_percentage`: Average of per-file CERs.
            - `total_files_evaluated`, `total_files_missing_ground_truth`.
        - `per_file_results`: A list of objects, each detailing:
            - `audio_file_name`
            - `wer_percentage`, `cer_percentage`
            - `status` (e.g., 'evaluated', 'missing_ground_truth')
            - `ground_truth_original`, `hypothesis_original`
            - `ground_truth_normalized`, `hypothesis_normalized`
            - `raw_metrics`: Dictionary containing counts for hits, s, d, i, and ground_truth_words for the file.
    - **Console Output:** A summary of global metrics and per-file WER/CER highlights.

## 3. Core Script
- The functionality is implemented in `scripts/evaluate_transcriptions.py`.

## 4. Usage Example
```bash
python scripts/evaluate_transcriptions.py \
    --ground_truth_file "path/to/ground_truth.json" \
    --hypotheses_input "path/to/hypotheses_dir_or_file.json" \
    --output_file "path/to/evaluation_report.json" \
    --log_level INFO
```

## 5. Key Decisions & Outcome
- The script successfully loads ground truth and hypothesis data.
- It performs text normalization.
- It calculates WER, CER, and a detailed breakdown of word-level errors (insertions, substitutions, deletions, hits) using `jiwer`.
- Global error rates are calculated based on total error counts and total ground truth words across all files.
- Per-file WER/CER are also reported.
- Output is provided as a detailed JSON file and a console summary.
- The initial consideration for CSV output was replaced by the more comprehensive JSON format.

## Target Audience

- Developers working on the Azure Speech Service integration.
- Data analysts assessing transcription quality.
- Product owners or stakeholders interested in understanding and improving speech-to-text performance.

## Open Questions

- What is the precise format for the input file/directory containing Azure transcription results? (Section 4.1 mentions "format to be decided").
- What are the specific text normalization rules to be applied consistently across both ground truth and hypothesis texts? (Section 4.2.4 mentions this is an "Important Consideration" needing detailed decisions).
- How should the script handle cases where an audio file is present in the Azure results but entirely missing from the ground truth, and vice-versa?
- For the initial version, is console output sufficient for the report, or is a specific file output (CSV/JSON) a hard requirement? (Section 3.2. mentions "Console printout, potentially a CSV or JSON file"). 