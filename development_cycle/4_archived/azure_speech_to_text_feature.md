# Feature: Azure Speech-to-Text Integration

**Goal:** Implement functionality to transcribe audio files using the Azure Speech-to-Text service.

**User Story:** As a user, I want to be able to submit an audio file to the application and receive its transcription generated by the Azure Speech-to-Text service, so that I can convert spoken content into text.

**Requirements/Prerequisites:**

1.  **Azure Subscription:** An active Azure subscription is required.
2.  **Azure AI Services Resource (Speech):** An AI Services resource for Speech must be created in the Azure portal.
3.  **Resource Key, Region, and Custom Model Endpoint ID:** Obtain the API key (Subscription Key), Azure region, and the Endpoint ID of your deployed custom speech model from the Azure portal / Speech Studio.
4.  **Python Environment:** A Python environment (3.7+) is needed.
5.  **Azure Speech SDK:** The `azure-cognitiveservices-speech` Python package must be installed (`pip install azure-cognitiveservices-speech`).
6.  **Audio File:** An audio file (e.g., WAV format) to be transcribed.

**Configuration:**

1.  **Environment Variables:** Store the Azure Speech resource credentials and custom model identifier securely as environment variables:
    *   `SPEECH_KEY`: Your Azure Speech resource subscription key.
    *   `SPEECH_REGION`: Your Azure Speech resource region (e.g., `westus`).
    *   `SPEECH_ENDPOINT_ID`: The Endpoint ID of your custom speech model.
    *   These should be loaded into the application's configuration from a `.env` file.

**Implementation Steps (Python SDK Approach):**

1.  **Import SDK:** Import necessary components from `azure.cognitiveservices.speech`.
2.  **Configure SpeechConfig for Custom Model:**
    *   Create an instance of `speechsdk.SpeechConfig` using the `SPEECH_KEY` (subscription key) and `SPEECH_REGION` from the environment variables.
    *   Set the `endpoint_id` property of the `speech_config` object to the `SPEECH_ENDPOINT_ID` from the environment variables.
    *   Example:
        ```python
        import azure.cognitiveservices.speech as speechsdk
        import os # Required for os.environ.get

        # Assuming SPEECH_KEY, SPEECH_REGION, SPEECH_ENDPOINT_ID are loaded from env
        # e.g., using a library like python-dotenv and os.environ
        speech_key = os.environ.get('SPEECH_KEY')
        speech_region = os.environ.get('SPEECH_REGION')
        speech_endpoint_id = os.environ.get('SPEECH_ENDPOINT_ID')

        speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=speech_region)
        speech_config.endpoint_id = speech_endpoint_id
        ```
3.  **Configure AudioConfig:** Create an instance of `speechsdk.audio.AudioConfig` pointing to the input audio file (e.g., `filename="path/to/your/audio.wav"`).
4.  **Create SpeechRecognizer:** Instantiate `speechsdk.SpeechRecognizer` using the `speech_config` and `audio_config`.
5.  **Perform Recognition:** Call the appropriate method for single-shot recognition from a file, like `recognize_once_async().get()`.
6.  **Process Result:**
    *   Check the `result.reason` to determine if recognition was successful (`speechsdk.ResultReason.RecognizedSpeech`), encountered silence (`speechsdk.ResultReason.NoMatch`), or was canceled (`speechsdk.ResultReason.Canceled`).
    *   If successful, extract the transcription text from `result.text`.
    *   If canceled, check `result.cancellation_details.reason` and `result.cancellation_details.error_details` for error information.

**Alternative Implementation (REST API):**

*   As described in the documentation, a POST request can be made to the service endpoint: `https://<REGION>.stt.speech.microsoft.com/speech/recognition/conversation/cognitiveservices/v1?language=<LANGUAGE>&format=detailed`.
*   Headers must include `Ocp-Apim-Subscription-Key` and `Content-Type` (e.g., `audio/wav`).
*   The audio file data is sent in the request body.
*   Refer to the `curl` examples in the documentation for structure.

**Output Format:**

*   The service returns a JSON response.
*   **SDK:** The `SpeechRecognitionResult` object contains properties like `text`, `reason`, `offset`, `duration`, and `cancellation_details`.
*   **REST API (detailed format):** The JSON includes `RecognitionStatus`, `DisplayText`, `Offset`, `Duration`, and potentially `NBest` alternatives.

**Error Handling:**

*   Handle potential exceptions during SDK calls (e.g., authentication errors, file not found).
*   Check the `result.reason` and `cancellation_details` for errors returned by the service.
*   Implement retries or specific error handling for API rate limits if using the REST API directly.
*   Ensure proper handling of different audio file formats (the SDK might require specific formats or additional dependencies like GStreamer for compressed audio).

**Acceptance Criteria:**

1.  The application can successfully authenticate with the Azure Speech service using configured credentials.
2.  Given a valid audio file path (e.g., WAV), the application sends the file to Azure Speech-to-Text.
3.  The application receives the transcription text from the service.
4.  The application correctly handles and reports errors (e.g., authentication failure, invalid file, service errors).
5.  Configuration (API key, region) is managed securely (e.g., via environment variables). 