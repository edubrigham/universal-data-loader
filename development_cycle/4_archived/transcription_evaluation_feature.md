# Feature Document: Transcription Quality Evaluation

- **Version:** 0.1
- **Date:** {{YYYY-MM-DD}}
- **Author(s):** {{Your Name/Team}}
- **Status:** In Planning

## 1. Introduction / Goals

This feature aims to provide a quantitative evaluation of the transcription quality produced by the Azure Speech Service. By comparing the service's output against a manually verified ground truth dataset, we can calculate standard industry metrics to assess accuracy. This will help in:
- Understanding the performance of the current transcription setup.
- Identifying areas for improvement (e.g., impact of audio quality, need for custom model fine-tuning).
- Tracking performance over time or across different configurations.

## 2. Scope

### 2.1. In Scope
- Reading transcription results generated by the `AzureSpeechService` (specifically, the text output).
- Reading ground truth transcriptions from the `evaluation-data/ground_truth_transcriptions.json` file.
- Matching Azure transcription results with the corresponding ground truth entries based on audio filenames.
- Calculating Word Error Rate (WER) for each transcribed file.
- Calculating Character Error Rate (CER) for each transcribed file.
- Calculating overall average WER and CER across a batch of evaluated files.
- Providing a report of these metrics, likely as console output or a simple text/JSON/CSV file.
- The evaluation will be performed by a Python script.

### 2.2. Out of Scope
- Real-time evaluation during transcription.
- Evaluation of aspects other than transcribed text accuracy (e.g., speaker diarization, punctuation accuracy beyond its impact on WER/CER, confidence scores analysis unless directly used in WER/CER calculation or filtering).
- Advanced statistical analysis or visualization of results (beyond basic reporting).
- A user interface for running evaluations or viewing results (initial version will be script-based).
- Automatic re-training or model adjustment based on evaluation results.

## 3. Requirements

### 3.1. Functional Requirements
- The system must be able to load ground truth data from `evaluation-data/ground_truth_transcriptions.json`.
- The system must be able to take as input one or more Azure transcription results (e.g., text from `AzureTranscriptionResult.text`).
- The system must correctly match the hypothesis text (Azure output) with the reference text (ground truth) using the audio filename as a key.
- The system must calculate WER between a hypothesis and reference string.
- The system must calculate CER between a hypothesis and reference string.
- The system must output per-file WER and CER.
- The system must output aggregate average WER and CER for a processed batch.
- If a ground truth entry for a given Azure transcription result is not found, this should be handled gracefully (e.g., skipped with a warning).

### 3.2. Data Requirements
- **Input 1 (Ground Truth):** `evaluation-data/ground_truth_transcriptions.json`
    - Format: JSON list of objects, each with `audio_file_name` (string) and `ground_truth_text` (string).
- **Input 2 (Hypotheses):** Transcription results from `AzureSpeechService`.
    - This will likely be a list of `AzureTranscriptionResult` objects or a similar structure containing at least the `original_file_path` (to derive filename) and the transcribed `text`.
- **Output (Evaluation Report):**
    - Per-file metrics: `audio_file_name`, `WER`, `CER`.
    - Aggregate metrics: `average_WER`, `average_CER`, `total_files_evaluated`, `total_files_missing_ground_truth`.
    - Format: Console printout, potentially a CSV or JSON file for easier parsing/storage.

## 4. Proposed Solution / Technical Design

### 4.1. Evaluation Script
- A new Python script will be created (e.g., `scripts/evaluate_transcriptions.py`).
- This script will accept:
    - Path to the ground truth JSON file.
    - Path to a file or directory containing Azure transcription results (format to be decided - could be a JSON file where each entry links filename to transcribed text, or direct output from a batch transcription).

### 4.2. Core Logic
1.  **Load Ground Truth:** Read `ground_truth_transcriptions.json` into a Python dictionary for easy lookup by `audio_file_name`.
2.  **Process Hypotheses:**
    - Iterate through the provided Azure transcription results.
    - For each result, extract the audio filename and the transcribed text.
3.  **Match and Compare:**
    - Look up the ground truth text for the current audio filename.
    - If found, calculate WER and CER.
    - If not found, log a warning and skip.
4.  **Text Normalization (Important Consideration):**
    - Before calculating WER/CER, both ground truth and hypothesis text should undergo a consistent normalization process. This typically includes:
        - Converting to lowercase.
        - Removing punctuation (or deciding on a consistent punctuation handling strategy).
        - Normalizing numbers (e.g., "two" vs "2").
        - Handling other common variations (e.g., expanding contractions like "it's" to "it is" or vice-versa, consistently).
    - The chosen Python library for WER/CER might offer some of these normalization options, or custom normalization functions will be needed.
5.  **Metrics Calculation:**
    - Utilize a well-tested Python library for WER/CER calculation. Recommended options:
        - `jiwer`: Popular, feature-rich library specifically for WER, MER, WIL, CER. Handles basic normalization.
        - `nltk.metrics.distance.edit_distance`: Can be used as a basis for CER and, with word tokenization, for WER.
    - WER = (Substitutions + Insertions + Deletions) / Number of Words in Reference
    - CER = (Substitutions + Insertions + Deletions) / Number of Characters in Reference

### 4.3. Output
- Print a summary to the console.
- Optionally, write detailed results to a CSV or JSON file.
    - CSV Example: `audio_file,wer,cer`
    - JSON Example: Similar to the ground truth structure but with added metrics.

## 5. Metrics for Evaluation

- **Primary Metrics:**
    - **Word Error Rate (WER):** Standard metric for speech recognition accuracy at the word level. Lower is better.
    - **Character Error Rate (CER):** Useful for languages where word tokenization is complex or for a finer-grained error analysis. Lower is better.
- **Secondary/Informational Metrics:**
    - Number of substitutions, insertions, deletions (at word and character levels) if provided by the chosen library.
    - Total files evaluated.
    - Number of files for which ground truth was not found.

## 6. Success Criteria / Acceptance Criteria

- The evaluation script runs without errors.
- Given a set of hypothesis transcriptions and corresponding ground truth:
    - The script correctly loads and matches them.
    - Calculated WER/CER values are accurate (can be verified with a small, manually calculated example).
    - The script produces an output report in the specified format containing per-file and aggregate metrics.
- Text normalization steps are applied consistently and are documented.
- Missing ground truth entries are handled and reported.

## 7. Deployment Plan Considerations

- **Dependencies:** The evaluation script will have dependencies on Python libraries (e.g., `jiwer` or `nltk`). These should be added to `requirements.txt` or `pyproject.toml`.
- **Execution:** The script will be run manually from the command line.
- **Input Data Flow:**
    - Ground truth JSON is manually maintained or generated.
    - Azure transcription results need to be collected (e.g., from the output of `transcribe_azure_cli.py`) and provided as input to the evaluation script. A standardized format for these results (if not already in a list of `AzureTranscriptionResult` like objects) might be needed.
- **Results Storage:** Initial results will be console output or local files. No central database or dashboard is planned for v0.1.

## 8. Risks and Assumptions

- **Assumption:** Ground truth data in `ground_truth_transcriptions.json` is accurate and meticulously verified.
- **Assumption:** Audio filenames are consistent between Azure transcription outputs and the ground truth file for matching.
- **Risk:** Inconsistent text normalization between hypothesis and reference can lead to misleading WER/CER scores. This needs careful implementation and testing.
- **Risk:** The chosen WER/CER library might have its own nuances or limitations.
- **Risk:** Large discrepancies in transcription length (e.g., Azure produces very little text for an audio file that has a long ground truth) might skew average metrics if not handled or noted.

## 9. Future Enhancements (Optional)

- Support for more sophisticated text normalization options.
- Generation of a diff-like output showing specific errors (substitutions, insertions, deletions).
- Integration with a more formal experiment tracking system.
- Visualization of results.
- Evaluation of other transcription aspects (e.g., punctuation, speaker labels if available).
- Parameterizing the evaluation script for different normalization strategies. 